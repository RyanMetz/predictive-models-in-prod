{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport pmip.feature_engineering\n",
    "%aimport pmip.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pmip.data import pickle_to_fs\n",
    "from pmip.feature_engineering import TextSelector, NumberSelector, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_DIR=os.path.join(\"..\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"ENVIRONMENT\", \"\") == \"dev\":\n",
    "    zip_archive = os.path.join(DATA_DIR, \"YouTube-Spam-Collection-v1.zip\")\n",
    "    if not os.path.isdir(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\", \n",
    "        zip_archive,\n",
    "    )\n",
    "    with zipfile.ZipFile(zip_archive, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "        \n",
    "training_files = os.listdir(DATA_DIR)\n",
    "training_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_list = []\n",
    "for file_ in [os.path.join(DATA_DIR, file) for file in training_files if file.endswith(\".csv\")]:\n",
    "    df = pd.read_csv(file_, index_col=None, header=0)\n",
    "    training_df_list.append(df)\n",
    "\n",
    "training_df = pd.concat(training_df_list, axis = 0, ignore_index = True)\n",
    "training_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(\n",
    "    training_df.CONTENT, \n",
    "    training_df.CLASS, \n",
    "    stratify=training_df.CLASS, \n",
    "    random_state=42, \n",
    "    test_size=0.1, \n",
    "    shuffle=True\n",
    ")\n",
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(\n",
    "    min_df=3,  \n",
    "    max_features=None,\n",
    "    strip_accents='unicode', \n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 3), \n",
    "    use_idf=1,\n",
    "    smooth_idf=1,\n",
    "    sublinear_tf=1,\n",
    "    stop_words = 'english'\n",
    ")\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVD\n",
    "svd = TruncatedSVD()\n",
    "    \n",
    "# Initialize the standard scaler \n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# We will use logistic regression here..\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                         ('scl', scl),\n",
    "                         ('lr', lr_model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120, 180],\n",
    "              'lr__C': [0.1, 1.0, 10], \n",
    "              'lr__penalty': ['l1', 'l2']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(\n",
    "    estimator=clf, \n",
    "    param_grid=param_grid, \n",
    "    verbose=10, \n",
    "    n_jobs=-1, \n",
    "    iid=True, \n",
    "    refit=True, \n",
    "    cv=2\n",
    ")\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_to_fs(model.best_estimator_, filename=\"model.pkl\", subdirectory=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
